{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Cleaning â€“ Credit Card Churn Dataset\n",
    "This notebook performs initial data cleaning on the raw credit card churn dataset.  \n",
    "The goal is to prepare the dataset for EDA and modeling by:\n",
    "- Removing duplicates\n",
    "- Handling missing values\n",
    "- Addressing outliers\n",
    "- Managing high-cardinality categorical features\n",
    "- Handle Imbalanced Target Variable (`AttritionFlag`)\n",
    "- Handle High Dimensionality\n",
    " \n",
    "The cleaned dataset will be saved in `data/processed/` for use in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"../../data/raw\")\n",
    "FILE_PATH = DATA_DIR / \"credit_card_attrition_dataset_mark.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1. Looking at the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "*I begin by examining the dataset, including number of rows, columns, data types, and summarize key statistics such as mean, median, min, max, and standard deviation for numeric columns. \n",
    "This helps to get an initial sense of the data and spot any obvious issues early on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "*Remove unnecessary columns like `CustomerID`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop CustomerID\n",
    "df.drop(columns=[\"CustomerID\"], inplace=True)\n",
    "\n",
    "print(\"CustomerID column removed. New shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 2. Checking for Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "*Duplicate records can skew the analysis and lead to biased models.  \n",
    "I check for exact duplicates in the dataset using `.duplicated()` method and remove them using `drop_duplicates()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique count for each variable\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 3. Checking for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Missing values can bias the model if not handled properly.  \n",
    "I check the distribution of missing values per column and decide on appropriate strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Income\", \"CreditLimit\", \"TotalSpend\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "*The columns that has missing values are `Income`, `CreditLimit`, `TotalSpend` which has 5k missing  values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = [\"Income\", \"CreditLimit\", \"TotalSpend\"]\n",
    "\n",
    "df[cols_with_missing].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "figures_dir = Path(\"../../reports/figures/data_cleaning\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cols_with_missing = [\"Income\", \"CreditLimit\", \"TotalSpend\"]\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(df[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col} (with Missing Values)\")\n",
    "    \n",
    "    # Save figure\n",
    "    save_path = figures_dir / f\"{col}_distribution.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "*Since `Income` and `TotalSpend` were highly right-skewed, I will use median imputation to avoid distortion from outliers. `CreditLimit` was nearly symmetric, so I will use mean imputation to preserve its distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median for skewed features\n",
    "df[\"Income\"] = df[\"Income\"].fillna(df[\"Income\"].median())\n",
    "df[\"TotalSpend\"] = df[\"TotalSpend\"].fillna(df[\"TotalSpend\"].median())\n",
    "\n",
    "# Mean for symmetric feature\n",
    "df[\"CreditLimit\"] = df[\"CreditLimit\"].fillna(df[\"CreditLimit\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Income\", \"CreditLimit\", \"TotalSpend\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 4. Checking for Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Outliers can distort model performance.  \n",
    "I will use boxplots and IQR method to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numeric columns except target\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "if \"AttritionFlag\" in numeric_cols:\n",
    "    numeric_cols.remove(\"AttritionFlag\")\n",
    "\n",
    "print(\"Numeric columns:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for detecting outliers using IQR\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"../../reports/figures/data_cleaning/outliers_before\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_summary.append({\n",
    "        \"Column\": col,\n",
    "        \"Num_Outliers\": len(outliers),\n",
    "        \"Lower_Bound\": lower,\n",
    "        \"Upper_Bound\": upper\n",
    "    })\n",
    "    \n",
    "    # Save boxplot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"{col} - Outlier Detection\")\n",
    "    plt.savefig(figures_dir / f\"{col}_boxplot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Summary DataFrame\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values(by=\"Num_Outliers\", ascending=False)\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound,\n",
    "                          np.where(df[column] > upper_bound, upper_bound, df[column]))\n",
    "\n",
    "# Apply to all numeric columns\n",
    "for col in numeric_cols:\n",
    "    cap_outliers_iqr(df, col)\n",
    "\n",
    "print(\"Outliers capped for all numeric columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"../../reports/figures/data_cleaning/outliers_after\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_summary.append({\n",
    "        \"Column\": col,\n",
    "        \"Num_Outliers\": len(outliers),\n",
    "        \"Lower_Bound\": lower,\n",
    "        \"Upper_Bound\": upper\n",
    "    })\n",
    "    \n",
    "    # Save boxplot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"{col} - Outlier Detection\")\n",
    "    plt.savefig(figures_dir / f\"{col}_boxplot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Summary DataFrame\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values(by=\"Num_Outliers\", ascending=False)\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "*I detected outliers using the IQR method and capped them instead of removing them to preserve dataset size while reducing the influence of extreme values. This is particularly important for financial datasets where high values can be genuine but shouldnâ€™t overly bias the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 5. Handling High Cardinality in Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Features with too many unique categories can cause problems for modeling, such as overfitting or high dimensionality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"Categorical columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in each categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "*The`Country` column is the only high cardinality, but we still need to convert categorical variables to numerical.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None) \n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"Distribution of {col}:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### *One hot encoding for low cardinality variables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Columns to one-hot encode\n",
    "low_cardinality_cols = [\"Gender\", \"MaritalStatus\", \"EducationLevel\", \"CardType\"]\n",
    "\n",
    "# One-hot encode\n",
    "df_encoded = pd.get_dummies(df, columns=low_cardinality_cols, drop_first=False)\n",
    "\n",
    "print(\"Shape before encoding:\", df.shape)\n",
    "print(\"Shape after encoding:\", df_encoded.shape)\n",
    "\n",
    "print(\"One-hot encoding done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "*Note: `df_encoded` is the new df we are working with.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### *Frequency Encoding for High Cardinality `Country` Column*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding for Country column\n",
    "country_freq = df_encoded['Country'].value_counts(normalize=False)  # counts (not percentage)\n",
    "\n",
    "# Map frequencies back to the dataframe\n",
    "df_encoded['Country_FE'] = df_encoded['Country'].map(country_freq)\n",
    "\n",
    "# Drop original Country column\n",
    "df_encoded.drop(columns=['Country'], inplace=True)\n",
    "\n",
    "print(\"Frequency encoding applied to Country column.\")\n",
    "print(df_encoded[['Country_FE']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "*The `df_encoded` is now fully numeric.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## 6. Identifying Imbalanced target variable `AttritionFlag`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Imbalanced classes can lead to biased models that favor the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check distribution\n",
    "class_counts = df_encoded['AttritionFlag'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.title(\"AttritionFlag Distribution\")\n",
    "plt.xlabel(\"AttritionFlag\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Note on Handling Class Imbalance\n",
    "\n",
    "The target variable `AttritionFlag` is imbalanced:\n",
    "\n",
    "- **Class 0:** 95,040 records  \n",
    "- **Class 1:** 4,960 records  \n",
    "\n",
    "I will **not** address this imbalance during the data cleaning phase for the following reasons:\n",
    "\n",
    "1. **Single Cleaned Dataset** â€“ I aim to produce **one cleaned CSV file** (`credit_card_cleaned.csv`) containing all rows and columns after cleaning, without any artificial oversampling or undersampling applied at this stage.  \n",
    "\n",
    "2. **Preventing Data Leakage** â€“ Oversampling techniques such as **SMOTE** must be applied **only to the training set**. Applying them before splitting the dataset could introduce information from the test set into the training process, leading to overly optimistic performance estimates.  \n",
    "\n",
    "3. **Better Workflow Separation** â€“ Class balancing will be handled in the **model preparation stage**, after the train-test split. The steps will be:\n",
    "   - Split the cleaned dataset into training and testing sets.\n",
    "   - Apply **SMOTE** only on the training set to generate synthetic samples for the minority class.\n",
    "   - Leave the test set untouched for an unbiased evaluation of the model.\n",
    "\n",
    "This approach ensures a clean, reusable dataset for multiple modeling experiments while preserving the integrity of model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## 7. Handling High dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "High dimensionality can lead to increased computational cost, model overfitting, and difficulty in interpreting results.  \n",
    "In this step, we aim to simplify the dataset while retaining useful information by applying two techniques:\n",
    "\n",
    "1. **Check for Near-Zero Variance Features**  \n",
    "   Features with little to no variance across samples carry minimal predictive power.  \n",
    "   Such columns provide almost the same value for all observations, making them unhelpful for distinguishing between classes.  \n",
    "   Removing them helps reduce noise and speeds up model training without sacrificing accuracy.\n",
    "\n",
    "2. **Remove Highly Correlated Features**  \n",
    "   Features with very high correlation (e.g., Pearson correlation coefficient > 0.95) are essentially redundant, as they carry the same information.  \n",
    "   Keeping both leads to:\n",
    "   - Multicollinearity issues in linear models (e.g., Logistic Regression), where coefficient estimates become unstable.\n",
    "   - Increased complexity in tree-based models (e.g., Random Forest, XGBoost), with no real gain in predictive power.\n",
    "   \n",
    "By performing these steps **now**, we ensure that the dataset is optimized for modeling without introducing data leakage.  \n",
    "No transformations here use the target variable (`AttritionFlag`), so the process is safe to apply before splitting the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## Identify & remove near-zero variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial shape:\", df_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude target\n",
    "target = \"AttritionFlag\"\n",
    "feature_cols = [c for c in df_encoded.columns if c != target]\n",
    "\n",
    "# Calculate stats for each feature\n",
    "nzv_stats = []\n",
    "for col in feature_cols:\n",
    "    top_freq = df_encoded[col].value_counts(normalize=True, dropna=False).iloc[0]\n",
    "    var = df_encoded[col].var()\n",
    "    nunique = df_encoded[col].nunique(dropna=False)\n",
    "    nzv_stats.append((col, top_freq, var, nunique))\n",
    "\n",
    "nzv_df = pd.DataFrame(nzv_stats, columns=[\"feature\",\"top_freq\",\"variance\",\"nunique\"])\n",
    "nzv_df = nzv_df.sort_values(\"top_freq\", ascending=False)\n",
    "\n",
    "# Show features where >99% of values are the same (adjust threshold if you want)\n",
    "top_freq_threshold = 0.99\n",
    "near_constant = nzv_df[nzv_df[\"top_freq\"] >= top_freq_threshold][\"feature\"].tolist()\n",
    "\n",
    "print(f\"Found {len(near_constant)} near-constant features (top_freq >= {top_freq_threshold}):\")\n",
    "print(near_constant)\n",
    "\n",
    "# Optionally also show very small variance features (useful for continuous)\n",
    "var_threshold = 1e-6   # you can tune this small number if needed\n",
    "low_variance = nzv_df[nzv_df[\"variance\"] <= var_threshold][\"feature\"].tolist()\n",
    "print(f\"\\nFound {len(low_variance)} very low variance features (variance <= {var_threshold}):\")\n",
    "print(low_variance)\n",
    "\n",
    "# Consolidate features to drop (union of both sets)\n",
    "to_drop_nzv = sorted(set(near_constant + low_variance))\n",
    "print(f\"\\nTotal features suggested to drop (near-zero var): {len(to_drop_nzv)}\")\n",
    "\n",
    "# Preview table for inspection\n",
    "display(nzv_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "*None of the features are useless constants, so we can skip removing near-zero variance columns*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### *Identify Highly Correlated Columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_encoded.corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "high_corr_cols = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
    "print(f\"Highly correlated columns ({len(high_corr_cols)}): {high_corr_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Renaming Encoded Gender Column\n",
    "\n",
    "`Gender_Female` and `Gender_Male` are highly correlated because they are direct complements (if one is 1, the other is 0). Therefore, we will drop `Gender_Male` and rename `Gender_Female` to `Is_Female` for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Gender_Male\n",
    "df_encoded = df_encoded.drop(columns=[\"Gender_Male\"])\n",
    "\n",
    "# Rename Gender_Female to Is_Female\n",
    "df_encoded = df_encoded.rename(columns={\"Gender_Female\": \"Is_Female\"})\n",
    "\n",
    "print(\"Dropped 'Gender_Male' and renamed 'Gender_Female' to 'Is_Female'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "In this step, only **1 column** (`Gender_Male`) was dropped due to perfect correlation with `Gender_Female`.  \n",
    "This leaves us with **71 columns** in the dataset.\n",
    "\n",
    "While 71 features are not considered high dimensional in a strict sense  \n",
    "(high dimensionality typically refers to datasets with hundreds or thousands of features),  \n",
    "this step was still important to:\n",
    "\n",
    "- Remove redundant variables to avoid multicollinearity, which can impact model interpretability (especially in Logistic Regression).  \n",
    "- Eliminate unnecessary complexity without losing useful information.\n",
    "\n",
    "No additional dimensionality reduction was performed at this stage.  \n",
    "Further dimensionality reduction techniques and feature selection will be considered later during the **feature engineering and modeling phase** using  \n",
    "model-based feature importance, domain knowledge, or dimensionality reduction techniques if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Saving the Cleaned Dataset\n",
    "\n",
    "We have now finalized the **data cleaning process** and will save the cleaned dataset into the `data/processed` folder.  \n",
    "This ensures that subsequent steps such as **feature engineering** and **model training** use a consistent, pre-cleaned dataset without repeating the cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset to processed folder\n",
    "processed_path = Path(\"../../data/processed/credit_card_attrition_cleaned.csv\")\n",
    "df_encoded.to_csv(processed_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {processed_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditcard-env",
   "language": "python",
   "name": "creditcard-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
