{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Cleaning â€“ Credit Card Churn Dataset\n",
    "This notebook performs initial data cleaning on the raw credit card churn dataset.  \n",
    "The goal is to prepare the dataset for EDA and modeling by:\n",
    "- Removing duplicates\n",
    "- Handling missing values\n",
    "- Addressing outliers\n",
    "- Managing high-cardinality categorical features\n",
    "- Handle Imbalanced Target Variable (`AttritionFlag`)\n",
    "- Handle High Dimensionality\n",
    " \n",
    "The cleaned dataset will be saved in `data/processed/` for use in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"../../data/raw\")\n",
    "FILE_PATH = DATA_DIR / \"credit_card_attrition_dataset_mark.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Looking at the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "*Remove unnecessary columns like `CustomerID`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop CustomerID\n",
    "df.drop(columns=[\"CustomerID\"], inplace=True)\n",
    "\n",
    "print(\"CustomerID column removed. New shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 2. Checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique count for each variable\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 3. Checking for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Income\", \"CreditLimit\", \"TotalSpend\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "*The columns that has missing values area `Income`, `CreditLimit`, `TotalSpend` which has 5k missing  values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = [\"Income\", \"CreditLimit\", \"TotalSpend\"]\n",
    "\n",
    "df[cols_with_missing].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "figures_dir = Path(\"../../reports/figures\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cols_with_missing = [\"Income\", \"CreditLimit\", \"TotalSpend\"]\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(df[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col} (with Missing Values)\")\n",
    "    \n",
    "    # Save figure\n",
    "    save_path = figures_dir / f\"{col}_distribution.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "*Since `Income` and `TotalSpend` were highly right-skewed, I will use median imputation to avoid distortion from outliers. `CreditLimit` was nearly symmetric, so I will use mean imputation to preserve its distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median for skewed features\n",
    "df[\"Income\"] = df[\"Income\"].fillna(df[\"Income\"].median())\n",
    "df[\"TotalSpend\"] = df[\"TotalSpend\"].fillna(df[\"TotalSpend\"].median())\n",
    "\n",
    "# Mean for symmetric feature\n",
    "df[\"CreditLimit\"] = df[\"CreditLimit\"].fillna(df[\"CreditLimit\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Income\", \"CreditLimit\", \"TotalSpend\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 4. Checking for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numeric columns except target\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "if \"AttritionFlag\" in numeric_cols:\n",
    "    numeric_cols.remove(\"AttritionFlag\")\n",
    "\n",
    "print(\"Numeric columns:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for detecting outliers using IQR\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"../../reports/figures/outliers_before\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_summary.append({\n",
    "        \"Column\": col,\n",
    "        \"Num_Outliers\": len(outliers),\n",
    "        \"Lower_Bound\": lower,\n",
    "        \"Upper_Bound\": upper\n",
    "    })\n",
    "    \n",
    "    # Save boxplot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"{col} - Outlier Detection\")\n",
    "    plt.savefig(figures_dir / f\"{col}_boxplot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# Summary DataFrame\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values(by=\"Num_Outliers\", ascending=False)\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound,\n",
    "                          np.where(df[column] > upper_bound, upper_bound, df[column]))\n",
    "\n",
    "# Apply to all numeric columns\n",
    "for col in numeric_cols:\n",
    "    cap_outliers_iqr(df, col)\n",
    "\n",
    "print(\"Outliers capped for all numeric columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"../../reports/figures/outliers_after\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_summary.append({\n",
    "        \"Column\": col,\n",
    "        \"Num_Outliers\": len(outliers),\n",
    "        \"Lower_Bound\": lower,\n",
    "        \"Upper_Bound\": upper\n",
    "    })\n",
    "    \n",
    "    # Save boxplot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"{col} - Outlier Detection\")\n",
    "    plt.savefig(figures_dir / f\"{col}_boxplot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# Summary DataFrame\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values(by=\"Num_Outliers\", ascending=False)\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "*I detected outliers using the IQR method and capped them instead of removing them to preserve dataset size while reducing the influence of extreme values. This is particularly important for financial datasets where high values can be genuine but shouldnâ€™t overly bias the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## 5. Handling High Cardinality in Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"Categorical columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in each categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "*The`Country` column is the only high cardinality, but we still need to convert categorical variables to numerical.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None) \n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"Distribution of {col}:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### *One hot encoding for low cardinality variables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Columns to one-hot encode\n",
    "low_cardinality_cols = [\"Gender\", \"MaritalStatus\", \"EducationLevel\", \"CardType\"]\n",
    "\n",
    "# One-hot encode\n",
    "df_encoded = pd.get_dummies(df, columns=low_cardinality_cols, drop_first=False)\n",
    "\n",
    "print(\"Shape before encoding:\", df.shape)\n",
    "print(\"Shape after encoding:\", df_encoded.shape)\n",
    "\n",
    "print(\"One-hot encoding done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "*Note: `df_encoded` is the new df we are working with.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### *Frequency Encoding for High Cardinality `Country` Column*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding for Country column\n",
    "country_freq = df_encoded['Country'].value_counts(normalize=False)  # counts (not percentage)\n",
    "\n",
    "# Map frequencies back to the dataframe\n",
    "df_encoded['Country_FE'] = df_encoded['Country'].map(country_freq)\n",
    "\n",
    "# Drop original Country column\n",
    "df_encoded.drop(columns=['Country'], inplace=True)\n",
    "\n",
    "print(\"Frequency encoding applied to Country column.\")\n",
    "print(df_encoded[['Country_FE']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditcard-env",
   "language": "python",
   "name": "creditcard-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
