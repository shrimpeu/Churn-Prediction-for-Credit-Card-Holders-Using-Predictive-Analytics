{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Cleaning â€“ Credit Card Churn Dataset\n",
    "This notebook performs initial data cleaning on the raw credit card churn dataset.  \n",
    "The goal is to prepare the dataset for EDA and modeling by:\n",
    "- Removing duplicates\n",
    "- Handling missing values\n",
    "- Fixing data types\n",
    "- Addressing outliers\n",
    "- Managing high-cardinality categorical features  \n",
    "The cleaned dataset will be saved in `data/processed/` for use in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"../../data/raw\")\n",
    "FILE_PATH = DATA_DIR / \"credit_card_attrition_dataset_mark.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Looking at the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 2. Checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 3. Checking for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Income\", \"CreditLimit\", \"TotalSpend\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "*The columns that has missing values area `Income`, `CreditLimit`, `TotalSpend` which has 5k missing  values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = [\"Income\", \"CreditLimit\", \"TotalSpend\"]\n",
    "\n",
    "df[cols_with_missing].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "figures_dir = Path(\"../../reports/figures\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cols_with_missing = [\"Income\", \"CreditLimit\", \"TotalSpend\"]\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(df[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col} (with Missing Values)\")\n",
    "    \n",
    "    # Save figure\n",
    "    save_path = figures_dir / f\"{col}_distribution.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "*Since `Income` and `TotalSpend` were highly right-skewed, I will use median imputation to avoid distortion from outliers. `CreditLimit` was nearly symmetric, so I will use mean imputation to preserve its distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median for skewed features\n",
    "df[\"Income\"] = df[\"Income\"].fillna(df[\"Income\"].median())\n",
    "df[\"TotalSpend\"] = df[\"TotalSpend\"].fillna(df[\"TotalSpend\"].median())\n",
    "\n",
    "# Mean for symmetric feature\n",
    "df[\"CreditLimit\"] = df[\"CreditLimit\"].fillna(df[\"CreditLimit\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Income\", \"CreditLimit\", \"TotalSpend\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 4. Checking for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"../../reports/figures/outliers\")\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for col in [\"Income\", \"CreditLimit\", \"TotalSpend\"]:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    sns.boxplot(x=df[col], ax=axes[0])\n",
    "    axes[0].set_title(f\"{col} Before Capping\")\n",
    "    \n",
    "    sns.histplot(df[col], kde=True, ax=axes[1])\n",
    "    axes[1].set_title(f\"{col} Distribution Before Capping\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / f\"{col}_before_capping.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cap_outliers_iqr(df, cols):\n",
    "    \"\"\"\n",
    "    Caps outliers in specified numeric columns using the IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataframe\n",
    "    cols (list): List of numeric columns to process\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Dataframe with capped outliers\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Cap values outside the bounds\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound,\n",
    "                           np.where(df[col] < lower_bound, lower_bound, df[col]))\n",
    "        \n",
    "        print(f\"{col}: capped values outside [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Identify numeric columns (excluding target)\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "numeric_cols.remove(\"AttritionFlag\")  # exclude target\n",
    "\n",
    "# Apply IQR capping\n",
    "df = cap_outliers_iqr(df, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# figures_dir = Path(\"../../reports/figures/outliers\")\n",
    "# figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for col in [\"Income\", \"CreditLimit\", \"TotalSpend\"]:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    sns.boxplot(x=df[col], ax=axes[0])\n",
    "    axes[0].set_title(f\"{col} After Capping\")\n",
    "    \n",
    "    sns.histplot(df[col], kde=True, ax=axes[1])\n",
    "    axes[1].set_title(f\"{col} Distribution After Capping\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(figures_dir / f\"{col}_after_capping.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "*I detected outliers using the IQR method and capped them instead of removing them to preserve dataset size while reducing the influence of extreme values. This is particularly important for financial datasets where high values can be genuine but shouldnâ€™t overly bias the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditcard-env",
   "language": "python",
   "name": "creditcard-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
