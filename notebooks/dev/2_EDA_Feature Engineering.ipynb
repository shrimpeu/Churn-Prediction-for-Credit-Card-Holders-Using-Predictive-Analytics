{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) and Feature Engineering\n",
    "\n",
    "In this notebook, I will perform exploratory data analysis (EDA) on the cleaned credit card customer data to better understand patterns, relationships, and distributions of variables that may influence customer churn.  \n",
    "\n",
    "Insights from EDA will guide the creation of meaningful features that could improve the performance of our predictive models.  \n",
    "\n",
    "### Note on Univariate Analysis\n",
    "\n",
    "Univariate analysis for both numerical and categorical features was already performed during the **Data Cleaning** stage:\n",
    "\n",
    "- **Numerical features:**  \n",
    "  - **Skewness:** Specifically examined `Income`, `CreditLimit`, and `TotalSpend`, as these columns contained missing values and required careful handling for imputation and potential transformation.  \n",
    "  - **Outliers:** All numerical features were checked for outliers using the **IQR method**. Outliers were **capped** rather than removed to preserve dataset size while reducing the influence of extreme values.\n",
    "\n",
    "- **Categorical features:**  \n",
    "  Frequency counts and distributions were assessed.  \n",
    "  - Low-cardinality features were **one-hot encoded**.  \n",
    "  - High-cardinality features were **frequency encoded** to retain predictive information without inflating dimensionality.\n",
    "\n",
    "As a result, the dataset has already been cleaned and transformed at a univariate level, allowing this notebook to focus on:\n",
    "\n",
    "- Bivariate analysis of features and the target variable  \n",
    "- Correlation analysis  \n",
    "- Feature Engineering\n",
    "- Encoding categorical variables and preparing data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset saved from the previous step\n",
    "cleaned_data_path = r'..\\..\\data\\processed\\credit_card_attrition_cleaned.csv'\n",
    "df = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Display the first few rows to verify loading\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Bivariate analysis of features and the target variable\n",
    "Goal: Identify features that differ significantly between churned (AttritionFlag=1) and non-churned (AttritionFlag=0) customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of churned and non-churned customers\n",
    "attrition_counts = df['AttritionFlag'].value_counts()\n",
    "\n",
    "# Pie chart\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(attrition_counts, labels=attrition_counts.index, autopct='%1.1f%%', startangle=90, colors=['#4CAF50', '#F44336'])\n",
    "plt.title('Distribution of AttritionFlag')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features by type\n",
    "continuous_features = ['Age', 'Income', 'CreditLimit', 'TotalSpend', 'Tenure', 'TotalTransactions']\n",
    "boolean_features = ['Is_Female', 'MaritalStatus_Divorced', 'MaritalStatus_Married', \n",
    "                    'MaritalStatus_Single', 'MaritalStatus_Widowed',\n",
    "                    'EducationLevel_Bachelor', 'EducationLevel_High School',\n",
    "                    'EducationLevel_Master', 'EducationLevel_PhD',\n",
    "                    'CardType_Black', 'CardType_Gold', 'CardType_Platinum', 'CardType_Silver']\n",
    "engineered_features = [f'Feature_{i}' for i in range(50)] + ['Country_FE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Continuous Features\n",
    "# -------------------------------\n",
    "\n",
    "print(\"Bivariate Analysis: Continuous Features\\n\")\n",
    "\n",
    "for col in continuous_features:\n",
    "    print(f\"{col} vs AttritionFlag\")\n",
    "    print(df.groupby('AttritionFlag')[col].describe(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set folder to save plots\n",
    "save_folder = r'..\\..\\reports\\figures\\EDA_FeatureEng\\Continuous_FeaturesVSAttritionFlag'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Number of rows and columns\n",
    "rows, cols = 2, 3\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4), sharex=True, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(continuous_features):\n",
    "    sns.boxplot(ax=axes[i], x='AttritionFlag', y=col, data=df)\n",
    "    axes[i].set_title(col, fontsize=10)\n",
    "    axes[i].set_xlabel(\"\")  # remove individual x-labels\n",
    "    axes[i].set_ylabel(\"\")  # remove individual y-labels\n",
    "\n",
    "# Shared axis labels\n",
    "fig.text(0.5, 0.04, 'AttritionFlag', ha='center', fontsize=12)\n",
    "fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n",
    "\n",
    "# Save single PNG\n",
    "plt.savefig(os.path.join(save_folder, 'continuous_features_boxplots.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "*For Continuous features `Age`, `Income`, `CreditLimit`, `TotalSpend`, `Tenure`, `TotalTransactions`,  there are no significant differences in the distributions of continuous features between customers who churned (AttritionFlag = 1) and those who did not (AttritionFlag = 0). This suggests that these continuous features may have limited predictive power for distinguishing churn in this dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Boolean Features - Print Crosstabs\n",
    "# -------------------------------\n",
    "print(\"Bivariate Analysis: Boolean Features\\n\")\n",
    "\n",
    "for col in boolean_features:\n",
    "    print(f'{col} vs AttritionFlag')\n",
    "    print(pd.crosstab(df[col], df['AttritionFlag'], normalize='columns'), \"\\n\")  # proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "save_folder = r'..\\..\\reports\\figures\\EDA_FeatureEng\\Boolean_FeaturesVSAttritionFlag'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "n_features = len(boolean_features)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4), sharey=True, sharex=True)\n",
    "axes = axes.flatten()  # Easier indexing\n",
    "\n",
    "for i, col in enumerate(boolean_features):\n",
    "    sns.countplot(x=col, hue='AttritionFlag', data=df, palette='Set2', ax=axes[i])\n",
    "    axes[i].set_title(f'{col} by AttritionFlag')\n",
    "    axes[i].set_xlabel(\"\")  # Remove X-axis label\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].legend(title='AttritionFlag', labels=['No Churn (0)', 'Churn (1)'])\n",
    "\n",
    "# Remove empty subplots but keep last one centered if applicable\n",
    "if n_features % n_cols != 0:\n",
    "    empty_plots = n_cols - (n_features % n_cols)\n",
    "    for j in range(1, empty_plots + 1):\n",
    "        fig.delaxes(axes[-j])  # Remove unused axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_folder, 'boolean_features_barplots.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "*Same with continuous variables, the boolean features do not show significant differences between Churn and No Churn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Engineered Features - Print Statistics\n",
    "# -------------------------------\n",
    "print(\"Bivariate Analysis: Engineered Features\\n\")\n",
    "\n",
    "for col in engineered_features:\n",
    "    print(f\"{col} vs AttritionFlag\")\n",
    "    print(df.groupby('AttritionFlag')[col].mean(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "save_folder = r'..\\..\\reports\\figures\\EDA_FeatureEng\\Engineered_FeaturesVSAttritionFlag'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "n_features = len(engineered_features)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4), sharey=False, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(engineered_features):\n",
    "    sns.boxplot(x='AttritionFlag', y=col, data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'{col} vs AttritionFlag')\n",
    "    axes[i].set_xlabel('')  # Remove X label\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "# Remove extra empty subplots\n",
    "if n_features % n_cols != 0:\n",
    "    empty_plots = n_cols - (n_features % n_cols)\n",
    "    for j in range(1, empty_plots + 1):\n",
    "        fig.delaxes(axes[-j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_folder, 'engineered_features_boxplots.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "*The bivariate analysis of the engineered features (`Feature_0` to `eature_49` and `Country_FE`) against the target variable AttritionFlag shows that the mean values of each feature are very similar between employees who stayed (`0`) and those who left (`1`).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Bivariate Analysis Conclusion\n",
    "The bivariate analysis shows minimal differences between the features and the target variable (`AttritionFlag`). Continuous, boolean, and engineered features all exhibit similar distributions across the target classes. Overall, no individual feature demonstrates a significant relationship with attrition, indicating that predictive patterns may require modeling feature interactions or more advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 2. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Goal: Discover highly correlated features to inform which features to combine, transform, or drop during feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation\n",
    "corr_matrix = df.corr(method='spearman')\n",
    "\n",
    "pd.set_option('display.max_rows', None) \n",
    "\n",
    "# Correlation with target\n",
    "target_corr = corr_matrix['AttritionFlag'].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=target_corr.index, y=target_corr.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Spearman Correlation with AttritionFlag')\n",
    "plt.title('Feature Correlation with Target')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "*Spearman correlation between all numeric features and `AttritionFlag` shows values extremely close to zero, indicating no significant linear or monotonic relationship. This aligns with the bivariate analysis findings, confirming that none of the features individually differentiate the target.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('Spearman Correlation Matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Correlation Analysis Conclusion\n",
    "*There are no notable correlations between feature-feature and feature-target relationships. The few negative correlations observed are expected, as they originate from individual columns that were one-hot encoded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Goal: To transform the existing dataset into a richer, more informative set of features that maximizes the modelâ€™s ability to predict attrition/churn, despite weak raw correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Create target-independent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['AvgTransaction'] = df['TotalSpend'] / np.where(df['TotalTransactions'] == 0, 1, df['TotalTransactions'])\n",
    "df['CreditUsage'] = df['TotalSpend'] / np.where(df['CreditLimit'] == 0, 1, df['CreditLimit'])\n",
    "df['SpendIncomeRatio'] = df['TotalSpend'] / np.where(df['Income'] == 0, 1, df['Income'])\n",
    "df['TenureRatio'] = df['Tenure'] / np.where(df['Age'] == 0, 1, df['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "`AvgTransaction`: Measures average spend per transaction, it normalizes spending by number of transactions to capture customer behavior.\n",
    "\n",
    "`CreditUsage`: Indicates how much of their credit limit the customer uses; can reflect financial stress or engagement with the product.\n",
    "\n",
    "`SpendIncomeRatio`: Shows the relative spending compared to income, which can indicate whether spending is sustainable or risky.\n",
    "\n",
    "`TenureRatio`: Normalizes tenure by age, capturing how long a customer has been with the bank relative to their age (loyalty vs. age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [f'Feature_{i}' for i in range(50)]\n",
    "\n",
    "df['Feature_sum'] = df[feature_cols].sum(axis=1)\n",
    "df['Feature_mean'] = df[feature_cols].mean(axis=1)\n",
    "df['Feature_std'] = df[feature_cols].std(axis=1)\n",
    "df['Feature_max'] = df[feature_cols].max(axis=1)\n",
    "df['Feature_min'] = df[feature_cols].min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Aggregates condense high-dimensional features into summary statistics:\n",
    "\n",
    "sum: total activity/score across all features\n",
    "\n",
    "mean: average behavior across features\n",
    "\n",
    "std: variability of behavior\n",
    "\n",
    "max/min: capture extremes or outliers\n",
    "\n",
    "Helps reduce dimensionality and captures overall trends in customer behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for skewness\n",
    "\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "num_cols.remove('AttritionFlag')  # exclude target\n",
    "\n",
    "skew_values = df[num_cols].skew().sort_values(ascending=False)\n",
    "print(skew_values.head(20))  # top 20 most skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['CreditUsage', 'SpendIncomeRatio']:  # only skewed features\n",
    "    min_val = df[col].min()\n",
    "    if min_val <= 0:\n",
    "        df[col + '_log'] = np.log1p(df[col] - min_val + 1)\n",
    "    else:\n",
    "        df[col + '_log'] = np.log1p(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "*Since `CreditUsage` and `SpendIncomeRatio` are highly-skewed, we perform a log transformation (with a shift if necessary) to reduce skewness and make the distributions more suitable for modeling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "*Checking if there are correlations between features with newly added ones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only numeric columns\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Compute correlation with target\n",
    "corr_with_target = df[numeric_cols].corr()['AttritionFlag'].sort_values(ascending=False)\n",
    "\n",
    "# Show top 10 positively and negatively correlated features\n",
    "print(\"Top positive correlations:\\n\", corr_with_target.head(10))\n",
    "print(\"\\nTop negative correlations:\\n\", corr_with_target.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_spearman = df.corr(method='spearman')['AttritionFlag'].sort_values(ascending=False)\n",
    "print(\"Top positive correlations:\\n\", corr_spearman.head(10))\n",
    "print(\"\\nTop negative correlations:\\n\", corr_spearman.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation\n",
    "corr_matrix = df.corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('Spearman Correlation Matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "All numeric features, including engineered ones, show very low correlations with the target (highest ~0.009, lowest ~-0.005).\n",
    "\n",
    "Even features with low correlation are kept, as they may still contribute in combination with others or capture non-linear relationships.\n",
    "\n",
    "This suggests that churn is likely driven by complex interactions, so feature engineering and non-linear models may be important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Create target-dependent features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### *Split dataset first to avoid data leakage since this will involve the target variable `AttritionFlag`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['AttritionFlag'])\n",
    "y = df['AttritionFlag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = X_train.join(y_train)\n",
    "\n",
    "# Example: churn rate per card type\n",
    "card_cols = ['CardType_Black', 'CardType_Gold', 'CardType_Platinum', 'CardType_Silver']\n",
    "for col in card_cols:\n",
    "    churn_rate = train_df.groupby(col)['AttritionFlag'].mean()\n",
    "    X_train[col + '_ChurnRate'] = X_train[col].map(churn_rate)\n",
    "    X_test[col + '_ChurnRate'] = X_test[col].map(churn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = ['TotalSpend', 'CreditUsage', 'AvgTransaction']\n",
    "mean_churned = train_df[train_df['AttritionFlag']==1][numeric_feats].mean()\n",
    "\n",
    "for feat in numeric_feats:\n",
    "    X_train[feat+'_diff_churn'] = X_train[feat] - mean_churned[feat]\n",
    "    X_test[feat+'_diff_churn'] = X_test[feat] - mean_churned[feat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_train and y_train for saving\n",
    "train_save = X_train.copy()\n",
    "train_save['AttritionFlag'] = y_train\n",
    "train_save.to_csv(r'..\\..\\data\\processed\\train.csv', index=False)\n",
    "\n",
    "# Combine X_test and y_test for saving\n",
    "test_save = X_test.copy()\n",
    "test_save['AttritionFlag'] = y_test\n",
    "test_save.to_csv(r'..\\..\\data\\processed\\test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_save.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "*After this code block, transfer to 3_ModelTraining.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## 4. Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check distribution\n",
    "class_counts = df['AttritionFlag'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.title(\"AttritionFlag Distribution\")\n",
    "plt.xlabel(\"AttritionFlag\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "*Since the target variable `AttritionFlag` is highly imbalance, we are going to apply SMOTE on the train data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution after SMOTE\n",
    "print(\"Training after SMOTE:\\n\", y_train_res.value_counts())\n",
    "print(\"Test distribution:\\n\", y_test.value_counts())\n",
    "\n",
    "# Remove scaling for XGBoost\n",
    "# (Tree models don't require standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Initialize model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,         # number of boosting rounds\n",
    "    learning_rate=0.1,        # step size shrinkage\n",
    "    max_depth=5,              # maximum tree depth\n",
    "    subsample=0.8,            # subsample ratio of training data\n",
    "    colsample_bytree=0.8,     # subsample ratio of features\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train on SMOTE-resampled data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check first 20 predicted probabilities\n",
    "print(y_pred_proba[:20])\n",
    "\n",
    "# Compare probability distribution for each class in test set\n",
    "print(\"\\nMean probability for non-churners (class 0):\", np.mean(y_pred_proba[y_test == 0]))\n",
    "print(\"Mean probability for churners (class 1):\", np.mean(y_pred_proba[y_test == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_res.value_counts())\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "print(\"Min prob:\", y_pred_proba.min(), \"Max prob:\", y_pred_proba.max())\n",
    "print(\"Mean prob for churners:\", y_pred_proba[y_test == 1].mean())\n",
    "print(\"Mean prob for non-churners:\", y_pred_proba[y_test == 0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train_res, y_train_res)\n",
    "print(classification_report(y_test, dummy.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importances = pd.Series(xgb_model.feature_importances_, index=X_train_res.columns)\n",
    "print(importances.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Quick churn vs non-churn mean comparison\n",
    "churn_means = X_train.groupby(y_train).mean()\n",
    "print(churn_means.T.sort_values(by=1, ascending=False).head(15))\n",
    "\n",
    "# Quick separation plot for a key numeric feature\n",
    "feature = \"CreditLimit\"  # change as needed\n",
    "plt.hist(X_train.loc[y_train == 0, feature], bins=30, alpha=0.5, label=\"Non-Churn\")\n",
    "plt.hist(X_train.loc[y_train == 1, feature], bins=30, alpha=0.5, label=\"Churn\")\n",
    "plt.legend()\n",
    "plt.title(f\"Distribution of {feature}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creditcard-env",
   "language": "python",
   "name": "creditcard-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
